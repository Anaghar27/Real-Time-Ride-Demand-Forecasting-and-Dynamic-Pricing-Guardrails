Tests conducted at the end of each phase:

Phase 0 test:

make setup
make up
make smoke
make check
make down

Phase 1 test:

# 0) Go to repo
cd "/Users/anaghar/Documents/Portfolio Projects/Real-Time-Ride-Demand-Forecasting-and-Dynamic-Pricing-Guardrails"

# 1) Start prerequisites
open -a Docker
until docker info >/dev/null 2>&1; do echo "Waiting for Docker..."; sleep 2; done
python3.11 --version
docker --version
docker compose version
make --version

# 2) Setup + platform
make setup
make up
make ps
curl -fsS http://localhost:8000/health
curl -fsS http://localhost:8000/ready

# 3) Timing helper
run_timed () {
  label="$1"; shift
  start=$(date +%s)
  echo "=== $label : START $(date) ==="
  "$@"
  rc=$?
  end=$(date +%s)
  echo "=== $label : END $(date) | elapsed_sec=$((end-start)) | rc=$rc ==="
  return $rc
}

# 4) Phase 1.1 -> 1.5 (required order)
run_timed "1.1 sample download" make ingest-sample-download
run_timed "1.3 zone dim load" make ingest-zone-dim
run_timed "1.2 raw sample load" make ingest-load-sample
run_timed "1.4 validation" make ingest-validate
run_timed "1.5 run sample" make ingest-run-sample
run_timed "1.5 rerun sample (idempotency)" make ingest-rerun-sample
run_timed "gate check" make ingest-gate-check

# 5) Verify 1.1-1.5 in DB
set -a; source .env; set +a
docker compose --env-file .env -f infra/docker-compose.yml exec -T postgres \
psql -U "$POSTGRES_USER" -d "$POSTGRES_DB" -c "
SELECT source_file,state,rows_read,rows_valid,rows_rejected,load_duration_sec,started_at,completed_at
FROM ingestion_batch_log
ORDER BY created_at DESC
LIMIT 20;"

docker compose --env-file .env -f infra/docker-compose.yml exec -T postgres \
psql -U "$POSTGRES_USER" -d "$POSTGRES_DB" -c "SELECT COUNT(*) AS raw_trips_count FROM raw_trips;"

docker compose --env-file .env -f infra/docker-compose.yml exec -T postgres \
psql -U "$POSTGRES_USER" -d "$POSTGRES_DB" -c "SELECT COUNT(*) AS failed_batches FROM ingestion_batch_log WHERE state='failed';"

# 6) Phase 1.6
run_timed "1.6 pilot backfill" make ingest-backfill-pilot
run_timed "1.6 full backfill" make ingest-backfill-full
run_timed "1.6 incremental backfill" make ingest-incremental

# 7) If full/incremental fails, inspect failing check(s)
docker compose --env-file .env -f infra/docker-compose.yml exec -T postgres \
psql -U "$POSTGRES_USER" -d "$POSTGRES_DB" -c "
SELECT batch_id, source_file, state, error_message, completed_at
FROM ingestion_batch_log
WHERE state='failed'
ORDER BY completed_at DESC;"

docker compose --env-file .env -f infra/docker-compose.yml exec -T postgres \
psql -U "$POSTGRES_USER" -d "$POSTGRES_DB" -c "
SELECT r.batch_id, r.check_name, r.passed, r.metric_value, r.threshold_value, r.details
FROM ingestion_check_results r
JOIN ingestion_batch_log b ON b.batch_id = r.batch_id
WHERE b.state='failed'
ORDER BY r.created_at DESC, r.check_name;"


Phase 2 test:

1. Run Phase 2 in required order:

make features-time-buckets
make features-aggregate
make features-calendar
make features-lag-roll
make features-validate
make features-publish

2. Confirm row-completeness (zones x 15-min buckets):

make db-shell

SELECT
  (SELECT COUNT(*) FROM fct_zone_demand_15m) AS actual_rows,
  (SELECT COUNT(*) FROM fct_zone_time_spine_15m) AS expected_rows;
Expected: actual_rows = expected_rows.

Or one-liner from shell:

docker compose --env-file .env -f infra/docker-compose.yml exec -T postgres \
psql -U ride_user -d ride_demand -c "SELECT (SELECT COUNT(*) FROM fct_zone_demand_15m) AS actual_rows, (SELECT COUNT(*) FROM fct_zone_time_spine_15m) AS expected_rows;"

3. Confirm uniqueness of final key:

SELECT zone_id, bucket_start_ts, COUNT(*) cnt
FROM fact_demand_features
GROUP BY zone_id, bucket_start_ts
HAVING COUNT(*) > 1;
Expected: no rows.

4. Confirm no leakage frame behavior (spot-check lags/rolling):

SELECT zone_id, bucket_start_ts, pickup_count, lag_1, lag_96, roll_mean_4
FROM fact_demand_features
ORDER BY zone_id, bucket_start_ts
LIMIT 200;
Expected: lag columns reflect prior buckets only; first buckets follow your null policy.

5. Confirm quality gate passed:

SELECT run_id, check_name, severity, passed, reason_code, creqated_at
FROM feature_check_results
ORDER BY created_at DESC
LIMIT 50;
Expected: all severity='critical' have passed=true.

6. Confirm publish metadata/logging:

SELECT run_id, feature_version, state, row_count, run_start_ts, run_end_ts, started_at, completed_at
FROM feature_batch_log
ORDER BY started_at DESC
LIMIT 20;
Expected: latest run is succeeded (or validated for dry-run), with nonzero row_count for publish.

7. Confirm rerun idempotency:

Re-run same interval/version:
make features-build

Re-check stable totals:

SELECT COUNT(*) AS rows, COALESCE(SUM(pickup_count),0) AS pickup_sum
FROM fact_demand_features
WHERE feature_version = 'v1';
Expected: metrics unchanged across reruns for same inputs.

8. Run tests:
.venv/bin/python -m pytest tests/unit/test_time_bucket_flooring.py tests/unit/test_calendar_feature_derivation.py tests/unit/test_lag_feature_correctness.py tests/unit/test_rolling_window_no_leakage.py tests/unit/test_null_policy.py
RUN_FEATURE_INTEGRATION=1 .venv/bin/python -m pytest test

RUN_FEATURE_INTEGRATION=1 .venv/bin/python -m pytest tests


Phase 3 test:

1. Install/update deps
.venv/bin/pip install -r requirements.txt

2. Static checks
.venv/bin/python -m ruff check src tests
.venv/bin/python -m mypy src

3. Run Phase 3 step-by-step (required sequence)
make eda-seasonality
make eda-sparsity
make eda-fallback-policy
make eda-docs
make eda-validate
make eda-run

4. Run Phase 3 tests
.venv/bin/python -m pytest tests/eda/test_seasonality_metrics.py tests/eda/test_sparsity_thresholds.py tests/eda/test_fallback_assignment.py tests/eda/test_assumptions_registry.py
RUN_EDA_INTEGRATION=1 .venv/bin/python -m pytest tests/eda/test_integration_eda_run.py
Optional full suite
RUN_FEATURE_INTEGRATION=1 RUN_EDA_INTEGRATION=1 .venv/bin/python -m pytest tests

5. DB verification queries (run in psql)
make db-shell
Then execute:

SELECT run_id, status, started_at, ended_at, feature_version, policy_version
FROM eda_run_log
ORDER BY started_at DESC
LIMIT 5;

SELECT run_id, check_name, severity, passed
FROM eda_check_results
ORDER BY created_at DESC
LIMIT 50;

SELECT run_id, COUNT(*) AS rows
FROM eda_time_profile_summary
GROUP BY run_id
ORDER BY MAX(created_at) DESC
LIMIT 5;

SELECT run_id, COUNT(*) AS rows, COUNT(DISTINCT zone_id) AS zones
FROM eda_zone_sparsity_summary
GROUP BY run_id
ORDER BY MAX(created_at) DESC
LIMIT 5;

SELECT run_id, COUNT(*) AS policy_rows, COUNT(DISTINCT zone_id) AS zones
FROM zone_fallback_policy
GROUP BY run_id
ORDER BY MAX(created_at) DESC
LIMIT 5;

SELECT s.run_id,
       COUNT(DISTINCT s.zone_id) AS sparse_zones,
       COUNT(DISTINCT p.zone_id) AS policy_zones
FROM eda_zone_sparsity_summary s
LEFT JOIN zone_fallback_policy p
  ON s.run_id = p.run_id AND s.zone_id = p.zone_id
GROUP BY s.run_id
ORDER BY s.run_id DESC
LIMIT 5;

6. Filesystem artifact checks
ls -la docs/eda
ls -la reports/eda
LATEST_RUN=$(ls -1 reports/eda | tail -n 1); echo "$LATEST_RUN"; ls -la "reports/eda/$LATEST_RUN"

7. Check per-run join consistency across all manual step-by-step execution:
export EDA_RUN_ID=$(python -c "import uuid; print(uuid.uuid4())")
make eda-seasonality
make eda-sparsity
make eda-fallback-policy
make eda-docs
make eda-validate

Phase 4 test:

make setup
make up

# Optional quick validation
.venv/bin/python -m ruff check src/training tests/training
.venv/bin/python -m pytest tests/training -q

# Phase 4 end-to-end
RUN_ID=$(python -c "import uuid; print(uuid.uuid4())"); echo $RUN_ID
make train-prepare-data RUN_ID=$RUN_ID
make train-show-splits RUN_ID=$RUN_ID
make train-baseline RUN_ID=$RUN_ID
make train-candidates RUN_ID=$RUN_ID
make train-compare RUN_ID=$RUN_ID
make train-track RUN_ID=$RUN_ID
make train-select-champion RUN_ID=$RUN_ID
make train-register-staging RUN_ID=$RUN_ID

# Full one-shot chain
make train-run-all

Scenario based testing:

make up
Use a fresh run id each time:

RUN_ID=$(python -c "import uuid; print(uuid.uuid4())"); echo $RUN_ID
Inspect outputs for every scenario:

cat reports/training/$RUN_ID/split_manifest.json
cat reports/training/$RUN_ID/metrics_summary.csv
cat reports/training/$RUN_ID/champion_decision.json

1. Scenario A: Zero-demand window (expect trivial metrics)
Set short window + current small split:

python - <<'PY'
import yaml
t=yaml.safe_load(open("configs/training.yaml"))
s=yaml.safe_load(open("configs/split_policy.yaml"))
t["data"]["start_date"]="2024-01-02"; t["data"]["end_date"]="2024-01-07"; t["data"]["feature_version"]="v1"
s["chronological_holdout"]={
 "train_start":"2024-01-02T00:00:00+00:00","train_end":"2024-01-05T00:00:00+00:00",
 "val_start":"2024-01-05T00:00:00+00:00","val_end":"2024-01-06T00:00:00+00:00",
 "test_start":"2024-01-06T00:00:00+00:00","test_end":"2024-01-08T00:00:00+00:00","gap_minutes":15}
s["rolling_origin"]={"enabled":True,"fold_count":2,"train_days":2,"val_days":1,"test_days":1,"stride_days":1,"gap_minutes":15}
yaml.safe_dump(t,open("configs/training.yaml","w"),sort_keys=False)
yaml.safe_dump(s,open("configs/split_policy.yaml","w"),sort_keys=False)
PY
RUN_ID=$(python -c "import uuid; print(uuid.uuid4())")
make train-run-all RUN_ID=$RUN_ID || true

Expected:
Many metrics 0.0
Gate may fail/pass depending policy
Useful to test failure handling, not model quality

2. Scenario B: Real mixed-demand window (primary quality test)
Build richer features first:

time make features-build FEATURE_START_DATE=2024-01-01 FEATURE_END_DATE=2024-03-31 FEATURE_VERSION=v1
Set training/split to same range:

python - <<'PY'
import yaml
t=yaml.safe_load(open("configs/training.yaml"))
s=yaml.safe_load(open("configs/split_policy.yaml"))
t["data"]["start_date"]="2024-01-01"; t["data"]["end_date"]="2024-03-31"; t["data"]["feature_version"]="v1"
s["chronological_holdout"]={
 "train_start":"2024-01-01T00:00:00+00:00","train_end":"2024-03-01T00:00:00+00:00",
 "val_start":"2024-03-01T00:00:00+00:00","val_end":"2024-03-16T00:00:00+00:00",
 "test_start":"2024-03-16T00:00:00+00:00","test_end":"2024-04-01T00:00:00+00:00","gap_minutes":15}
s["rolling_origin"]={"enabled":True,"fold_count":3,"train_days":30,"val_days":7,"test_days":7,"stride_days":7,"gap_minutes":15}
yaml.safe_dump(t,open("configs/training.yaml","w"),sort_keys=False)
yaml.safe_dump(s,open("configs/split_policy.yaml","w"),sort_keys=False)
PY
RUN_ID=$(python -c "import uuid; print(uuid.uuid4())")
make train-run-all RUN_ID=$RUN_ID

Expected:
Non-zero MAE/RMSE/WAPE/sMAPE
Meaningful baseline vs candidate differences
Most representative scenario

3. Scenario C: Sparse-zone stress
Use sparse zones only:

python - <<'PY'
import yaml
t=yaml.safe_load(open("configs/training.yaml"))
t["data"]["zones"]="4,12,13,232,256"  # replace with sparse IDs from your policy table if needed
yaml.safe_dump(t,open("configs/training.yaml","w"),sort_keys=False)
PY
RUN_ID=$(python -c "import uuid; print(uuid.uuid4())")
make train-run-all RUN_ID=$RUN_ID || true

Expected:
Sparse slice metrics degrade
Possible gate fail on sparse regression

4. Scenario D: Latency gate fail
Force strict latency threshold:

python - <<'PY'
import yaml
c=yaml.safe_load(open("configs/champion_policy.yaml"))
c["gate"]["max_latency_ms_per_row"]=0.000001
yaml.safe_dump(c,open("configs/champion_policy.yaml","w"),sort_keys=False)
PY
RUN_ID=$(python -c "import uuid; print(uuid.uuid4())")
make train-run-all RUN_ID=$RUN_ID || true

Expected:
champion_decision.json includes latency_exceeds_limit

5. Scenario E: Stability gate fail
Force very strict stability:

python - <<'PY'
import yaml
c=yaml.safe_load(open("configs/champion_policy.yaml"))
c["gate"]["max_stability_std_wape"]=0.0001
yaml.safe_dump(c,open("configs/champion_policy.yaml","w"),sort_keys=False)
PY
RUN_ID=$(python -c "import uuid; print(uuid.uuid4())")
make train-run-all RUN_ID=$RUN_ID || true

Expected:
reason_codes includes stability_threshold_failed

6. Scenario F: Baseline-dominant fail
Require huge improvement over baseline:

python - <<'PY'
import yaml
c=yaml.safe_load(open("configs/champion_policy.yaml"))
c["gate"]["min_improvement_over_baseline_pct"]=0.50
yaml.safe_dump(c,open("configs/champion_policy.yaml","w"),sort_keys=False)
PY
RUN_ID=$(python -c "import uuid; print(uuid.uuid4())")
make train-run-all RUN_ID=$RUN_ID || true

Expected:
reason_codes includes did_not_beat_baseline

7. Scenario G: Registration blocked on gate fail
Run selection then register explicitly:

RUN_ID=$(python -c "import uuid; print(uuid.uuid4())")
make train-baseline RUN_ID=$RUN_ID
make train-candidates RUN_ID=$RUN_ID
make train-select-champion RUN_ID=$RUN_ID
make train-register-staging RUN_ID=$RUN_ID || true

Expected:
Registration command exits non-zero if gate failed
model_registry_audit gets failure record

8. Scenario H: Full pass to staging
Relax only what’s needed for your data:

python - <<'PY'
import yaml
c=yaml.safe_load(open("configs/champion_policy.yaml"))
c["gate"]["max_stability_std_wape"]=2000
c["gate"]["max_latency_ms_per_row"]=5.0
c["gate"]["min_improvement_over_baseline_pct"]=0.02
yaml.safe_dump(c,open("configs/champion_policy.yaml","w"),sort_keys=False)
PY
RUN_ID=$(python -c "import uuid; print(uuid.uuid4())")
make train-run-all RUN_ID=$RUN_ID

Expected:
gate_passed: true
Model registered in MLflow Model Registry at Staging

To test the entire pipeline from Phase 2 to Phase 4:
RUN_ID=$(python -c "import uuid; print(uuid.uuid4())"); echo $RUN_ID
make train-run-all RUN_ID=$RUN_ID

Since the dataset is picked dynamically based on the dataset present at the time of model training, running the below commands to verify Phase 4:

make up
make smoke

# 1) Phase 1 (ONLY if you ingested new raw trip data)
make ingest-zone-dim
make ingest-incremental    # or: make ingest-run-sample
make ingest-validate

# 2) Get the auto-resolved window (from configs/training.yaml)
read START_DATE END_DATE <<< "$(.venv/bin/python -c 'from src.training.training_config import load_training_bundle; _, cfg, *_ = load_training_bundle(); print(cfg["data"]["start_date"], cfg["data"]["end_date"])')"
echo "Using window: $START_DATE -> $END_DATE"

# 3) Phase 2: build/publish features for that window
make features-build FEATURE_START_DATE=$START_DATE FEATURE_END_DATE=$END_DATE FEATURE_VERSION=v1

# 4) Phase 3: EDA (creates/updates zone_fallback_policy for p1)
make eda-run EDA_START_DATE=$START_DATE EDA_END_DATE=$END_DATE EDA_FEATURE_VERSION=v1 EDA_POLICY_VERSION=p1

# 5) Phase 4: train
RUN_ID=$(python -c "import uuid; print(uuid.uuid4())"); echo $RUN_ID
make train-run-all RUN_ID=$RUN_ID

make train-auto RUN_ID=$RUN_ID

Model evaluation flow:
- The pipeline evaluates baselines and candidates on the same holdout window and writes their metrics to a leaderboard; the key metric is WAPE (lower is better).
- Champion selection then does: best baseline = lowest WAPE among model_role=baseline, best candidate = lowest WAPE among model_role=candidate, and applies the “gate” rules in champion_policy.yaml (line 1) via select_champion.py (line 34).
- The gate requires (at minimum) the best candidate to beat the best baseline by ≥ 2% relative WAPE improvement, plus it must not regress too much on the sparse_zones slice, must meet latency/stability limits, and must have an mlflow_run_id (select_champion.py (line 34)).
- If the gate passes, that best candidate is registered as the new model version in MLflow (register_model.py (line 105)); if it fails, registration is blocked and you keep the existing champion.
- In your latest output, linear_elasticnet is the champion because it had the lowest candidate WAPE and gate_passed: true, so it was registered as ride-demand-forecast-model version 4 in Staging.


Phase 5 test:

1) Bring stack up

make setup
make up
make smoke
2) Build upstream (Phase 2–4) for the latest training window

read START_DATE END_DATE <<< "$(.venv/bin/python -c 'from src.training.training_config import load_training_bundle; _, cfg, *_ = load_training_bundle(); print(cfg["data"]["start_date"], cfg["data"]["end_date"])')"
echo "Window: $START_DATE -> $END_DATE"

make features-build FEATURE_START_DATE=$START_DATE FEATURE_END_DATE=$END_DATE FEATURE_VERSION=v1
make eda-run EDA_START_DATE=$START_DATE EDA_END_DATE=$END_DATE EDA_FEATURE_VERSION=v1 EDA_POLICY_VERSION=p1

RUN_ID=$(python -c "import uuid; print(uuid.uuid4())")
make train-auto RUN_ID=$RUN_ID
3) Compute aligned Phase 5 backfill window (1 hour)

read SCORE_START SCORE_END <<< "$(END_DATE="$END_DATE" .venv/bin/python - <<'PY'
import os
from datetime import datetime, timedelta, timezone
end_date = datetime.fromisoformat(os.environ["END_DATE"]).replace(tzinfo=timezone.utc)
start = end_date + timedelta(days=1)
end = start + timedelta(hours=1)
print(start.isoformat(), end.isoformat())
PY
)"
echo "Scoring window: $SCORE_START -> $SCORE_END"
4) Phase 5 validate-only

RUN_ID=$(python -c "import uuid; print(uuid.uuid4())")
SCORING_FORECAST_START_TS="$SCORE_START" \
SCORING_FORECAST_END_TS="$SCORE_END" \
make score-validate RUN_ID=$RUN_ID
5) Phase 5 write run

RUN_ID=$(python -c "import uuid; print(uuid.uuid4())")
make score-run-window RUN_ID=$RUN_ID \
  SCORE_FORECAST_START_TS="$SCORE_START" \
  SCORE_FORECAST_END_TS="$SCORE_END"
6) Verify run statuses

.venv/bin/python - <<'PY'
from sqlalchemy import text
from src.common.db import engine
q = """
SELECT run_id,status,started_at,forecast_start_ts,forecast_end_ts,row_count,failure_reason
FROM scoring_run_log
ORDER BY started_at DESC
LIMIT 10
"""
with engine.begin() as c:
    for r in c.execute(text(q)).mappings():
        print(dict(r))
PY
7) Verify forecast rows were written

.venv/bin/python - <<'PY'
from sqlalchemy import text
from src.common.db import engine
q = """
SELECT forecast_run_key, COUNT(*) AS rows, MIN(bucket_start_ts) AS min_ts, MAX(bucket_start_ts) AS max_ts
FROM demand_forecast
GROUP BY forecast_run_key
ORDER BY max_ts DESC
LIMIT 5
"""
with engine.begin() as c:
    for r in c.execute(text(q)).mappings():
        print(dict(r))
PY
8) Verify artifacts

ls -lt reports/scoring | head
9) (Optional) Verify scheduler

make score-schedule
Keep it running in terminal 1. In terminal 2:

while true; do
  clear
  date
  .venv/bin/python - <<'PY'
from sqlalchemy import text
from src.common.db import engine
q = """
SELECT run_id,status,started_at,forecast_start_ts,failure_reason
FROM scoring_run_log
ORDER BY started_at DESC
LIMIT 5
"""
with engine.begin() as c:
    for r in c.execute(text(q)).mappings():
        print(dict(r))
PY
  sleep 30
done
Stop with Ctrl+C.