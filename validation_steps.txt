Tests conducted at the end of each phase:

Phase 0 test:

make setup
make up
make smoke
make check
make down

Phase 1 test:

# 0) Go to repo
cd "/Users/anaghar/Documents/Portfolio Projects/Real-Time-Ride-Demand-Forecasting-and-Dynamic-Pricing-Guardrails"

# 1) Start prerequisites
open -a Docker
until docker info >/dev/null 2>&1; do echo "Waiting for Docker..."; sleep 2; done
python3.11 --version
docker --version
docker compose version
make --version

# 2) Setup + platform
make setup
make up
make ps
curl -fsS http://localhost:8000/health
curl -fsS http://localhost:8000/ready

# 3) Timing helper
run_timed () {
  label="$1"; shift
  start=$(date +%s)
  echo "=== $label : START $(date) ==="
  "$@"
  rc=$?
  end=$(date +%s)
  echo "=== $label : END $(date) | elapsed_sec=$((end-start)) | rc=$rc ==="
  return $rc
}

# 4) Phase 1.1 -> 1.5 (required order)
run_timed "1.1 sample download" make ingest-sample-download
run_timed "1.3 zone dim load" make ingest-zone-dim
run_timed "1.2 raw sample load" make ingest-load-sample
run_timed "1.4 validation" make ingest-validate
run_timed "1.5 run sample" make ingest-run-sample
run_timed "1.5 rerun sample (idempotency)" make ingest-rerun-sample
run_timed "gate check" make ingest-gate-check

# 5) Verify 1.1-1.5 in DB
set -a; source .env; set +a
docker compose --env-file .env -f infra/docker-compose.yml exec -T postgres \
psql -U "$POSTGRES_USER" -d "$POSTGRES_DB" -c "
SELECT source_file,state,rows_read,rows_valid,rows_rejected,load_duration_sec,started_at,completed_at
FROM ingestion_batch_log
ORDER BY created_at DESC
LIMIT 20;"

docker compose --env-file .env -f infra/docker-compose.yml exec -T postgres \
psql -U "$POSTGRES_USER" -d "$POSTGRES_DB" -c "SELECT COUNT(*) AS raw_trips_count FROM raw_trips;"

docker compose --env-file .env -f infra/docker-compose.yml exec -T postgres \
psql -U "$POSTGRES_USER" -d "$POSTGRES_DB" -c "SELECT COUNT(*) AS failed_batches FROM ingestion_batch_log WHERE state='failed';"

# 6) Phase 1.6
run_timed "1.6 pilot backfill" make ingest-backfill-pilot
run_timed "1.6 full backfill" make ingest-backfill-full
run_timed "1.6 incremental backfill" make ingest-incremental

# 7) If full/incremental fails, inspect failing check(s)
docker compose --env-file .env -f infra/docker-compose.yml exec -T postgres \
psql -U "$POSTGRES_USER" -d "$POSTGRES_DB" -c "
SELECT batch_id, source_file, state, error_message, completed_at
FROM ingestion_batch_log
WHERE state='failed'
ORDER BY completed_at DESC;"

docker compose --env-file .env -f infra/docker-compose.yml exec -T postgres \
psql -U "$POSTGRES_USER" -d "$POSTGRES_DB" -c "
SELECT r.batch_id, r.check_name, r.passed, r.metric_value, r.threshold_value, r.details
FROM ingestion_check_results r
JOIN ingestion_batch_log b ON b.batch_id = r.batch_id
WHERE b.state='failed'
ORDER BY r.created_at DESC, r.check_name;"


Phase 2 test:

1. Run Phase 2 in required order:

make features-time-buckets
make features-aggregate
make features-calendar
make features-lag-roll
make features-validate
make features-publish

2. Confirm row-completeness (zones x 15-min buckets):

make db-shell

SELECT
  (SELECT COUNT(*) FROM fct_zone_demand_15m) AS actual_rows,
  (SELECT COUNT(*) FROM fct_zone_time_spine_15m) AS expected_rows;
Expected: actual_rows = expected_rows.

Or one-liner from shell:

docker compose --env-file .env -f infra/docker-compose.yml exec -T postgres \
psql -U ride_user -d ride_demand -c "SELECT (SELECT COUNT(*) FROM fct_zone_demand_15m) AS actual_rows, (SELECT COUNT(*) FROM fct_zone_time_spine_15m) AS expected_rows;"

3. Confirm uniqueness of final key:

SELECT zone_id, bucket_start_ts, COUNT(*) cnt
FROM fact_demand_features
GROUP BY zone_id, bucket_start_ts
HAVING COUNT(*) > 1;
Expected: no rows.

4. Confirm no leakage frame behavior (spot-check lags/rolling):

SELECT zone_id, bucket_start_ts, pickup_count, lag_1, lag_96, roll_mean_4
FROM fact_demand_features
ORDER BY zone_id, bucket_start_ts
LIMIT 200;
Expected: lag columns reflect prior buckets only; first buckets follow your null policy.

5. Confirm quality gate passed:

SELECT run_id, check_name, severity, passed, reason_code, creqated_at
FROM feature_check_results
ORDER BY created_at DESC
LIMIT 50;
Expected: all severity='critical' have passed=true.

6. Confirm publish metadata/logging:

SELECT run_id, feature_version, state, row_count, run_start_ts, run_end_ts, started_at, completed_at
FROM feature_batch_log
ORDER BY started_at DESC
LIMIT 20;
Expected: latest run is succeeded (or validated for dry-run), with nonzero row_count for publish.

7. Confirm rerun idempotency:

Re-run same interval/version:
make features-build

Re-check stable totals:

SELECT COUNT(*) AS rows, COALESCE(SUM(pickup_count),0) AS pickup_sum
FROM fact_demand_features
WHERE feature_version = 'v1';
Expected: metrics unchanged across reruns for same inputs.

8. Run tests:
.venv/bin/python -m pytest tests/unit/test_time_bucket_flooring.py tests/unit/test_calendar_feature_derivation.py tests/unit/test_lag_feature_correctness.py tests/unit/test_rolling_window_no_leakage.py tests/unit/test_null_policy.py
RUN_FEATURE_INTEGRATION=1 .venv/bin/python -m pytest test

RUN_FEATURE_INTEGRATION=1 .venv/bin/python -m pytest tests


Phase 3 test:

1. Install/update deps
.venv/bin/pip install -r requirements.txt

2. Static checks
.venv/bin/python -m ruff check src tests
.venv/bin/python -m mypy src

3. Run Phase 3 step-by-step (required sequence)
make eda-seasonality
make eda-sparsity
make eda-fallback-policy
make eda-docs
make eda-validate
make eda-run

4. Run Phase 3 tests
.venv/bin/python -m pytest tests/eda/test_seasonality_metrics.py tests/eda/test_sparsity_thresholds.py tests/eda/test_fallback_assignment.py tests/eda/test_assumptions_registry.py
RUN_EDA_INTEGRATION=1 .venv/bin/python -m pytest tests/eda/test_integration_eda_run.py
Optional full suite
RUN_FEATURE_INTEGRATION=1 RUN_EDA_INTEGRATION=1 .venv/bin/python -m pytest tests

5. DB verification queries (run in psql)
make db-shell
Then execute:

SELECT run_id, status, started_at, ended_at, feature_version, policy_version
FROM eda_run_log
ORDER BY started_at DESC
LIMIT 5;

SELECT run_id, check_name, severity, passed
FROM eda_check_results
ORDER BY created_at DESC
LIMIT 50;

SELECT run_id, COUNT(*) AS rows
FROM eda_time_profile_summary
GROUP BY run_id
ORDER BY MAX(created_at) DESC
LIMIT 5;

SELECT run_id, COUNT(*) AS rows, COUNT(DISTINCT zone_id) AS zones
FROM eda_zone_sparsity_summary
GROUP BY run_id
ORDER BY MAX(created_at) DESC
LIMIT 5;

SELECT run_id, COUNT(*) AS policy_rows, COUNT(DISTINCT zone_id) AS zones
FROM zone_fallback_policy
GROUP BY run_id
ORDER BY MAX(created_at) DESC
LIMIT 5;

SELECT s.run_id,
       COUNT(DISTINCT s.zone_id) AS sparse_zones,
       COUNT(DISTINCT p.zone_id) AS policy_zones
FROM eda_zone_sparsity_summary s
LEFT JOIN zone_fallback_policy p
  ON s.run_id = p.run_id AND s.zone_id = p.zone_id
GROUP BY s.run_id
ORDER BY s.run_id DESC
LIMIT 5;

6. Filesystem artifact checks
ls -la docs/eda
ls -la reports/eda
LATEST_RUN=$(ls -1 reports/eda | tail -n 1); echo "$LATEST_RUN"; ls -la "reports/eda/$LATEST_RUN"

7. Check per-run join consistency across all manual step-by-step execution:
export EDA_RUN_ID=$(python -c "import uuid; print(uuid.uuid4())")
make eda-seasonality
make eda-sparsity
make eda-fallback-policy
make eda-docs
make eda-validate